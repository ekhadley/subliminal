so what now?

Still failing to get transfer for a model that has an SAE available.
I tried with llama-3-8b-it beucase it has an sae on sae-lens. nothing.
tried more stuff with gemma, including finetuning the teacher model that generates the numbers.
I found a non-neuronpedia sae from a reputable team for mistral-7b-Instruct-v0.1, so maybe that?

The question is how long should i wait before saying fuck it and trying to train my own sae? 
 - really dont like this idea.
 - Partly because im expecting saes to not be able to find the subliminal messages.
    - And if i trained the sae its hard to rule out skill issue and justify retraining it.
    - Also training sae on chat model has obvious hurdles. 
        - How much pretraining dataset vs chat dataset should be used?